{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "                help=\"path to Caffe 'deploy' prototxt file\")\n",
    "ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "                help=\"path to Caffe pre-trained model\")\n",
    "ap.add_argument(\"-v\", \"--video\", required=True,\n",
    "                help=\"path to input video file\")\n",
    "ap.add_argument(\"-l\", \"--label\", required=True,\n",
    "                help=\"class label we are interested in detecting + tracking\")\n",
    "ap.add_argument(\"-o\", \"--output\", type=str,\n",
    "                help=\"path to optional output video file\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2,\n",
    "                help=\"minimum probability to filter weak detections\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototxt_arg = 'MobileNetSSD_deploy.prototxt'\n",
    "model_arg = 'MobileNetSSD_deploy.caffemodel'\n",
    "video_arg = 'example_01.mp4'\n",
    "label_arg = 'person'\n",
    "output_arg = ''\n",
    "confidence_arg = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n"
     ]
    }
   ],
   "source": [
    "# initialize the list of class labels MobileNet SSD was trained to\n",
    "# detect\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "           \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "           \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "           \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "# net = cv.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "net = cv.dnn.readNetFromCaffe(prototxt_arg, model_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting video stream...\n"
     ]
    }
   ],
   "source": [
    "# initialize the video stream, dlib correlation tracker, output video\n",
    "# writer, and predicted class label\n",
    "print(\"[INFO] starting video stream...\")\n",
    "# vs = cv.VideoCapture(args[\"video\"])\n",
    "vs = cv.VideoCapture(video_arg)\n",
    "tracker = None\n",
    "writer = None\n",
    "label = \"\"\n",
    "# start the frames per second throughput estimator\n",
    "fps = FPS().start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] elapsed time: 18.33\n",
      "[INFO] approx. FPS: 32.58\n"
     ]
    }
   ],
   "source": [
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    \n",
    "    # grab the next frame from the video file\n",
    "    (grabbed, frame) = vs.read()\n",
    "    \n",
    "    # check to see if we have reached the end of the video file\n",
    "    if frame is None:\n",
    "        break\n",
    "        \n",
    "    # resize the frame for faster processing and then convert the\n",
    "    # frame from BGR to RGB ordering (dlib needs RGB ordering)\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "    # if we are supposed to be writing a video to disk, initialize\n",
    "    # the writer\n",
    "    # if args[\"output\"] is not None and writer is None:\n",
    "    if output_arg is not None and writer is None:\n",
    "        fourcc = cv.VideoWriter_fourcc(*\"MJPG\")\n",
    "        # writer = cv.VideoWriter(args[\"output\"], fourcc, 30, (frame.shape[1], frame.shape[0]), True)\n",
    "        writer = cv.VideoWriter(output_arg, fourcc, 30, (frame.shape[1], frame.shape[0]), True)\n",
    "    \n",
    "    # if our correlation object tracker is None we first need to\n",
    "    # apply an object detector to seed the tracker with something\n",
    "    # to actually track\n",
    "    if tracker is None:\n",
    "        # grab the frame dimensions and convert the frame to a blob\n",
    "        (h, w) = frame.shape[:2]\n",
    "        blob = cv.dnn.blobFromImage(frame, 0.007843, (w, h), 127.5)\n",
    "        \n",
    "        # pass the blob through the network and obtain the detections\n",
    "        # and predictions\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        \n",
    "        # ensure at least one detection is made\n",
    "        if len(detections) > 0:\n",
    "            # find the index of the detection with the largest\n",
    "            # probability -- out of convenience we are only going\n",
    "            # to track the first object we find with the largest\n",
    "            # probability; future examples will demonstrate how to\n",
    "            # detect and extract *specific* objects\n",
    "            i = np.argmax(detections[0, 0, :, 2])\n",
    "            \n",
    "            # grab the probability associated with the object along\n",
    "            # with its class label\n",
    "            conf = detections[0, 0, i, 2]\n",
    "            label = CLASSES[int(detections[0, 0, i, 1])]\n",
    "            \n",
    "            # filter out weak detections by requiring a minimum\n",
    "            # confidence\n",
    "            # if conf > args[\"confidence\"] and label == args[\"label\"]:\n",
    "            if conf > confidence_arg and label == label_arg:\n",
    "                # compute the (x, y)-coordinates of the bounding box\n",
    "                # for the object\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                \n",
    "                # construct a dlib rectangle object from the bounding\n",
    "                # box coordinates and then start the dlib correlation\n",
    "                # tracker\n",
    "                tracker = dlib.correlation_tracker()\n",
    "                rect = dlib.rectangle(startX, startY, endX, endY)\n",
    "                tracker.start_track(rgb, rect)\n",
    "                \n",
    "                # draw the bounding box and text for the object\n",
    "                cv.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "                cv.putText(frame, label, (startX, startY - 15),\n",
    "                           cv.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "                \n",
    "    # otherwise, we've already performed detection so let's track\n",
    "    # the object\n",
    "    else:\n",
    "        # update the tracker and grab the position of the tracked\n",
    "        # object\n",
    "        tracker.update(rgb)\n",
    "        pos = tracker.get_position()\n",
    "        \n",
    "        # unpack the position object\n",
    "        startX = int(pos.left())\n",
    "        startY = int(pos.top())\n",
    "        endX = int(pos.right())\n",
    "        endY = int(pos.bottom())\n",
    "        \n",
    "        # draw the bounding box from the correlation object tracker\n",
    "        cv.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "        cv.putText(frame, label, (startX, startY - 15),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "        \n",
    "    # check to see if we should write the frame to disk\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "        \n",
    "    # show the output frame\n",
    "    cv.imshow(\"Frame\", frame)\n",
    "    key = cv.waitKey(1) & 0xFF\n",
    "    \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    \n",
    "    # update the FPS counter\n",
    "    fps.update()\n",
    "    \n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# check to see if we need to release the video writer pointer\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv.destroyAllWindows()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
