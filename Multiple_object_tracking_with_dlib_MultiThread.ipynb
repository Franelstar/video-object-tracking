{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils.video import FPS\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_tracker(box, label, rgb, inputQueue, outputQueue):\n",
    "    # construct a dlib rectangle object from the bounding box\n",
    "    # coordinates and then start the correlation tracker\n",
    "    t = dlib.correlation_tracker()\n",
    "    rect = dlib.rectangle(box[0], box[1], box[2], box[3])\n",
    "    t.start_track(rgb, rect)\n",
    "    \n",
    "    # loop indefinitely -- this function will be called as a daemon\n",
    "    # process so we don't need to worry about joining it\n",
    "    while True:\n",
    "        # attempt to grab the next frame from the input queue\n",
    "        rgb = inputQueue.get()\n",
    "        \n",
    "        # if there was an entry in our queue, process it\n",
    "        if rgb is not None:\n",
    "            # update the tracker and grab the position of the tracked object\n",
    "            t.update(rgb)\n",
    "            pos = t.get_position()\n",
    "            \n",
    "            # unpack the position object\n",
    "            startX = int(pos.left())\n",
    "            startY = int(pos.top())\n",
    "            endX = int(pos.right())\n",
    "            endY = int(pos.bottom())\n",
    "            \n",
    "            # add the label + bounding box coordinates to the output queue\n",
    "            outputQueue.put((label, (startX, startY, endX, endY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] -p PROTOTXT -m MODEL -v VIDEO [-o OUTPUT]\n",
      "                             [-c CONFIDENCE]\n",
      "ipykernel_launcher.py: error: the following arguments are required: -p/--prototxt, -m/--model, -v/--video\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franel/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "                help=\"path to Caffe 'deploy' prototxt file\")\n",
    "ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "                help=\"path to Caffe pre-trained model\")\n",
    "ap.add_argument(\"-v\", \"--video\", required=True,\n",
    "                help=\"path to input video file\")\n",
    "ap.add_argument(\"-o\", \"--output\", type=str,\n",
    "                help=\"path to optional output video file\")\n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2,\n",
    "                help=\"minimum probability to filter weak detections\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototxt_arg = 'MobileNetSSD_deploy.prototxt'\n",
    "model_arg = 'MobileNetSSD_deploy.caffemodel'\n",
    "video_arg = 'Pexels Videos 2670.mp4'\n",
    "label_arg = 'person'\n",
    "output_arg = ''\n",
    "confidence_arg = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our lists of queues -- both input queue and output queue\n",
    "# for *every* object that we will be tracking\n",
    "inputQueues = []\n",
    "outputQueues = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n",
      "[INFO] starting video stream...\n"
     ]
    }
   ],
   "source": [
    "# initialize the list of class labels MobileNet SSD was trained to detect\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "           \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "           \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "           \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "# net = cv.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "net = cv.dnn.readNetFromCaffe(prototxt_arg, model_arg)\n",
    "\n",
    "# initialize the video stream and output video writer\n",
    "print(\"[INFO] starting video stream...\")\n",
    "# vs = cv.VideoCapture(args[\"video\"])\n",
    "vs = cv.VideoCapture(video_arg)\n",
    "writer = None\n",
    "\n",
    "# start the frames per second throughput estimator\n",
    "fps = FPS().start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not yet\n",
      "not yet\n",
      "not yet\n",
      "not yet\n",
      "[INFO] elapsed time: 32.74\n",
      "[INFO] approx. FPS: 10.42\n"
     ]
    }
   ],
   "source": [
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    # grab the next frame from the video file\n",
    "    (grabbed, frame) = vs.read()\n",
    "    # check to see if we have reached the end of the video file\n",
    "    if frame is None:\n",
    "        break\n",
    "        \n",
    "    # resize the frame for faster processing and then convert the\n",
    "    # frame from BGR to RGB ordering (dlib needs RGB ordering)\n",
    "    frame = imutils.resize(frame, width=600)\n",
    "    rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "    # if we are supposed to be writing a video to disk, initialize\n",
    "    # the writer\n",
    "    # if args[\"output\"] is not None and writer is None:\n",
    "    if output_arg is not None and writer is None:\n",
    "        fourcc = cv.VideoWriter_fourcc(*\"MJPG\")\n",
    "        # writer = cv.VideoWriter(args[\"output\"], fourcc, 30, (frame.shape[1], frame.shape[0]), True)\n",
    "        writer = cv.VideoWriter(output_arg, fourcc, 30, (frame.shape[1], frame.shape[0]), True)\n",
    "    \n",
    "    # if our list of queues is empty then we know we have yet to\n",
    "    # create our first object tracker\n",
    "    if len(inputQueues) == 0:\n",
    "        # grab the frame dimensions and convert the frame to a blob\n",
    "        (h, w) = frame.shape[:2]\n",
    "        blob = cv.dnn.blobFromImage(frame, 0.007843, (w, h), 127.5)\n",
    "        \n",
    "        # pass the blob through the network and obtain the detections\n",
    "        # and predictions\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        \n",
    "        # loop over the detections\n",
    "        for i in np.arange(0, detections.shape[2]):\n",
    "            # extract the confidence (i.e., probability) associated\n",
    "            # with the prediction\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            \n",
    "            # filter out weak detections by requiring a minimum confidence\n",
    "            # if confidence > args[\"confidence\"]:\n",
    "            if confidence > confidence_arg:\n",
    "                # extract the index of the class label from the\n",
    "                # detections list\n",
    "                idx = int(detections[0, 0, i, 1])\n",
    "                label = CLASSES[idx]\n",
    "                \n",
    "                # if the class label is not a person, ignore it\n",
    "                if CLASSES[idx] != label_arg:\n",
    "                    print('not yet')\n",
    "                    continue\n",
    "                \n",
    "                # compute the (x, y)-coordinates of the bounding box\n",
    "                # for the object\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                bb = (startX, startY, endX, endY)\n",
    "                \n",
    "                # create two brand new input and output queues, respectively\n",
    "                iq = multiprocessing.Queue()\n",
    "                oq = multiprocessing.Queue()\n",
    "                inputQueues.append(iq)\n",
    "                outputQueues.append(oq)\n",
    "\n",
    "                # spawn a daemon process for a new object tracker\n",
    "                p = multiprocessing.Process(\n",
    "                    target=start_tracker,\n",
    "                    args=(bb, label, rgb, iq, oq))\n",
    "                p.daemon = True\n",
    "                p.start()\n",
    "                \n",
    "                # grab the corresponding class label for the detection\n",
    "                # and draw the bounding box\n",
    "                cv.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "                cv.putText(frame, label, (startX, startY - 15),\n",
    "                           cv.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "                \n",
    "    # otherwise, we've already performed detection so let's track\n",
    "    # multiple objects\n",
    "    else:\n",
    "        # loop over each of our input ques and add the input RGB\n",
    "        # frame to it, enabling us to update each of the respective\n",
    "        # object trackers running in separate processes\n",
    "        for iq in inputQueues:\n",
    "            iq.put(rgb)\n",
    "            \n",
    "        # loop over each of the output queues\n",
    "        for oq in outputQueues:\n",
    "            # grab the updated bounding box coordinates for the\n",
    "            # object -- the .get method is a blocking operation so\n",
    "            # this will pause our execution until the respective\n",
    "            # process finishes the tracking update\n",
    "            (label, (startX, startY, endX, endY)) = oq.get()\n",
    "            \n",
    "            # draw the bounding box from the correlation object tracker\n",
    "            cv.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "            cv.putText(frame, label, (startX, startY - 15),\n",
    "                       cv.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "            \n",
    "    # check to see if we should write the frame to disk\n",
    "    if writer is not None:\n",
    "        writer.write(frame)\n",
    "        \n",
    "    # show the output frame\n",
    "    cv.imshow(\"Frame\", frame)\n",
    "    key = cv.waitKey(1) & 0xFF\n",
    "    \n",
    "    # if the `q` key was pressed, break from the loop\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    \n",
    "    # update the FPS counter\n",
    "    fps.update()\n",
    "\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# check to see if we need to release the video writer pointer\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv.destroyAllWindows()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
